---
title: "Logistic Regression Tutorial"
author: "Rick Scavetta"
date: "26 November 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

From: datascience+[http://datascienceplus.com/perform-logistic-regression-in-r/]

## Load the data

Use ``na.strings= ""` to force empty cells to be `NA`. If you don't do this characters will remain as empty cells, so the downstream functions will not work. 

```{r load}
training.data.raw <- read.csv('train.csv', na.strings= "")
```

## Check data

Check for missing values and look how many unique values there are for each variable using the sapply() function which applies the function passed as argument to each column of the dataframe.

```{r}
sapply(training.data.raw,function(x) sum(is.na(x)))
```

```{r}
sapply(training.data.raw, function(x) length(unique(x)))
```

## First plots:

The `Amelia` package has a special plotting function `missmap()` that will plot your dataset and highlight missing values



```{r pressure, echo=FALSE}
library(Amelia)
missmap(training.data.raw, main = "Missing values vs observed")
```

The variable cabin has too many missing values, we will not use it. We will also drop PassengerId since it is only an index and Ticket.

## Filter

Using the `subset()` function we subset the original dataset selecting the relevant columns only.

```{r}
data <- subset(training.data.raw,select=c(2,3,5,6,7,8,10,12))
```

## Imputation

Now we need to account for the other missing values. R can easily deal with them when fitting a generalized linear model by setting a parameter inside the fitting function. However, personally I prefer to replace the NAs “by hand”, when is possible. There are different ways to do this, a typical approach is to replace the missing values with the average, the median or the mode of the existing one. I’ll be using the average.


```{r}
data$Age[is.na(data$Age)] <- mean(data$Age,na.rm=T)
```


As far as categorical variables are concerned, using the read.table() or read.csv() by default will encode the categorical variables as factors. A factor is how R deals categorical variables.
We can check the encoding using the following lines of code

```{r}
is.factor(data$Sex)
is.factor(data$Embarked)
```

## Dummy variables

For a better understanding of how R is going to deal with the categorical variables, we can use the `contrasts()` function. This function will show us how the variables have been dummyfied by R and how to interpret them in a model.

```{r}
contrasts(data$Sex)
contrasts(data$Embarked)
```

For instance, you can see that in the variable sex, female will be used as the reference. As for the missing values in Embarked, since there are only two, we will discard those two rows (we could also have replaced the missing values with the mode and keep the datapoints).

```{r}
data <- data[!is.na(data$Embarked),]
rownames(data) <- NULL
```

Before proceeding to the fitting process, let me remind you how important is cleaning and formatting of the data. This preprocessing step often is crucial for obtaining a good fit of the model and better predictive ability.

## Model fitting

We split the data into two chunks: training and testing set. The training set will be used to fit our model which we will be testing over the testing set.


```{r}
train <- data[1:800,]
test <- data[801:889,]
```

Now, let’s fit the model. Be sure to specify the parameter `family=binomial` in the `glm()` function.

```{r}
model <- glm(Survived ~.,family=binomial(link='logit'),data=train)
```

By using function `summary()` we obtain the results of our model:

```{r}
summary(model)
```

## Interpreting the results of our logistic regression model

Now we can analyze the fitting and interpret what the model is telling us.

First of all, we can see that SibSp, Fare and Embarked are not statistically significant. As for the statistically significant variables, sex has the lowest p-value suggesting a strong association of the sex of the passenger with the probability of having survived. The negative coefficient for this predictor suggests that all other variables being equal, the male passenger is less likely to have survived.

Remember that in the logit model the response variable is log odds: `ln(odds) = ln(p/(1-p)) = a*x1 + b*x2 + ... + z*xn`. 

Since male is a dummy variable, being male reduces the log odds by 2.75 while a unit increase in age reduces the log odds by 0.037.

Now we can run the `anova()` function on the model to analyze the table of deviance.

```{r}
anova(model, test="Chisq")
```

The difference between the null deviance and the residual deviance shows how our model is doing against the null model (a model with only the intercept). The wider this gap, the better. Analyzing the table we can see the drop in deviance when adding each variable one at a time. Again, adding Pclass, Sex and Age significantly reduces the residual deviance. The other variables seem to improve the model less even though SibSp has a low p-value. A large p-value here indicates that the model without the variable explains more or less the same amount of variation. Ultimately what you would like to see is a significant drop in deviance and the AIC.

While no exact equivalent to the `R2` of linear regression exists, the McFadden `R2` index can be used to assess the model fit.

```{r}
library(pscl)
pR2(model)
```

## Assessing the predictive ability of the model

In the steps above, we briefly evaluated the fitting of the model, now we would like to see how the model is doing when predicting y on a new set of data. By setting the parameter `type='response'`, R will output probabilities in the form of `P(y=1|X)`. Our decision boundary will be `0.5`. If `P(y=1|X) > 0.5` then `y = 1` otherwise `y=0`. Note that for some applications different decision boundaries could be a better option.

```{r}
fitted.results <- predict(model,newdata=subset(test,select=c(2,3,4,5,6,7,8)),type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)

misClasificError <- mean(fitted.results != test$Survived)
print(paste('Accuracy',1-misClasificError))
```

The 0.84 accuracy on the test set is quite a good result. However, keep in mind that this result is somewhat dependent on the manual split of the data that I made earlier, therefore if you wish for a more precise score, you would be better off running some kind of cross validation such as k-fold cross validation.

## ROC & AUC

As a last step, we are going to plot the ROC curve and calculate the AUC (area under the curve) which are typical performance measurements for a binary classifier.

The ROC is a curve generated by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings while the AUC is the area under the ROC curve. As a rule of thumb, a model with good predictive ability should have an AUC closer to 1 (1 is ideal) than to 0.5.

```{r}
library(ROCR)
p <- predict(model, newdata=subset(test,select=c(2,3,4,5,6,7,8)), type="response")
pr <- prediction(p, test$Survived)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
```
